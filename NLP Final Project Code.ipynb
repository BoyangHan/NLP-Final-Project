{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Cleaning & Word Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_cleaning(description, details, brand, product_full_name, idf_score):\n",
    "    \"\"\"\n",
    "    Obtain a TFIDF vector for a given product\n",
    "    \n",
    "    Parameters:\n",
    "    arg1(string): The description of the product\n",
    "    arg2(string): The details of the product\n",
    "    arg3(string): The full_name of the product\n",
    "    arg4(string): IDF score calculated from the training data\n",
    "    \n",
    "    Return the TFIDF vector\n",
    "    \"\"\"\n",
    "    ## Function to remove stopwords\n",
    "    def remove_stopwords(title: str, stopword_list):\n",
    "        tokens = nltk.word_tokenize(title)\n",
    "        filtered_tokens = list(filter(lambda token: token not in stopword_list, tokens))\n",
    "        return \" \".join(filtered_tokens)\n",
    "    ## Function to remove special characters\n",
    "    def removeSpecialChars(z):\n",
    "        return z.translate ({ord(c): \"\" for c in \"!@#$%^&*()[]{};:,./'<>?\\|`~-=_+\"})\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ## Function to convert nltk tag to wordnet tag\n",
    "    def nltk2wn_tag(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "    ## Function to lemmatize\n",
    "    def lemmatize_sentence(sentence):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        wn_tagged = map(lambda x :(x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "        res_words = []\n",
    "        for word, tag in wn_tagged:\n",
    "            if tag is None:\n",
    "                res_words.append(word)\n",
    "            else:\n",
    "                res_words.append(lemmatizer.lemmatize(word,tag))\n",
    "        return \" \".join(res_words)\n",
    "    ## Difine a function to calculate the term frequency array\n",
    "    def all_list(arr):\n",
    "        result = {}\n",
    "        for i in set(arr):\n",
    "            result[i] = arr.count(i)\n",
    "        return result\n",
    "    \n",
    "    ## Define different stopwords lists for three different inputs\n",
    "    stopword = set(stopwords.words('english'))\n",
    "    stopword_des = stopwords.words('english') + ['.' , '', ]\n",
    "    stopword_de = stopwords.words('english') + [\"\", \"h\", \"l\", \"mm\", \"w\", \"x\", \"xl\", \"xs\", \"cm\", \"uv\"]\n",
    "    stopword_br = stopwords.words('english') + ['by', 'and', 'the', 'of', 'for', 'to', 'inc', '', 'a', 'an']\n",
    "    stopword_pro = stopwords.words('english') + ['&','|',',','.',\"'\",'in','le','mm','and','with','the','k','x','les','de','of',\"'s\"]\n",
    "    \n",
    "    # Regax cleaning for description\n",
    "    des = re.sub(r'[^\\w+0-9%.]', ' ', description, flags = re.IGNORECASE)\n",
    "    ## Replace all numbers with % with text description (HIGH/MEDIAN/LOW)\n",
    "    perc = re.findall(r'(\\d+)(?:%)\\s', des)\n",
    "    temp_content = \"\"\n",
    "    temp = []\n",
    "    if perc != []:\n",
    "        for i in perc:\n",
    "            if int(i) >= 80:\n",
    "                temp_content = temp_content + \"HIGH\"\n",
    "            elif int(i) >= 40:\n",
    "                temp_content = temp_content + \"MEDIAN\"\n",
    "            else:\n",
    "                temp_content = temp_content + \"LOW\"\n",
    "            temp.append(temp_content)\n",
    "            temp_content = \"\"\n",
    "    des = re.sub(r\"(\\d+)(?:%)\\s(\\w+)\", \"MATERIAL\", des, flags = re.IGNORECASE)\n",
    "    flag = 0\n",
    "    temp_list = []\n",
    "    for word in des.split(\" \"):\n",
    "        if word == \"MATERIAL\":\n",
    "            temp_list.append(\"{}_MATERIAL\".format(temp[flag]))\n",
    "            flag += 1\n",
    "        else:\n",
    "            temp_list.append(word)\n",
    "    temp_des = \" \".join(temp_list)\n",
    "    des = re.sub(r\"((\\d+\\.\\d+)|(\\d+))\", \"NUMERICAL_VALUE\", temp_des, flags = re.IGNORECASE)\n",
    "    ## Remove stopwords, unnecessary punctuations and lower cases\n",
    "    updated_des = []\n",
    "    temp_des = []\n",
    "    line = re.findall(r'\\b[a-zA-Z0-9_]{2,}\\b',des)\n",
    "    for word in line:\n",
    "        if word.lower() in stopword_des:\n",
    "            continue\n",
    "        temp_des.append(word.lower())\n",
    "    updated_des = \" \".join(temp_des)\n",
    "    ## Lemmatize the description\n",
    "    updated_des = lemmatize_sentence(updated_des)\n",
    "    \n",
    "    # Regax cleaning for details\n",
    "    ## Remove all the numbers punctuations and stopwords\n",
    "    details = details.lower()\n",
    "    details = re.sub(r\"[^a-z\\s]+\", \"\", details, flags = re.IGNORECASE)\n",
    "    details = re.sub(r\"\\r\\n\", \"\", details, flags = re.IGNORECASE)\n",
    "    updated_details = remove_stopwords(details, stopword_de)\n",
    "    ## Lemmatize the details\n",
    "    updated_details = lemmatize_sentence(updated_details)\n",
    "    \n",
    "    # Regax cleaning for brand\n",
    "    ## Lower cases and remove special characters\n",
    "    brand = brand.lower()\n",
    "    brand = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", brand)\n",
    "    brand = removeSpecialChars(brand)\n",
    "    ## Remove stopwords\n",
    "    updated_brand = remove_stopwords(brand, stopword_br)\n",
    "    ## Lemmatize the brand\n",
    "    updated_brand = lemmatize_sentence(updated_brand)\n",
    "    \n",
    "    # Regax cleaning for product full name\n",
    "    ## Lower cases\n",
    "    product_full_name = product_full_name.lower()\n",
    "    ## Remove hyphen\n",
    "    product_full_name = re.sub(\"-\", \"\", product_full_name, flags = re.IGNORECASE)\n",
    "    ## Remove numbers\n",
    "    product_full_name = re.sub(r'[0-9]+',\"\", product_full_name, flags = re.IGNORECASE)\n",
    "    ## Remove stopwords\n",
    "    updated_product_full_name = remove_stopwords(product_full_name, stopword_pro)\n",
    "    ## Lemmatize the brand\n",
    "    updated_product_full_name = lemmatize_sentence(updated_product_full_name)\n",
    "    \n",
    "    # Join the four features together\n",
    "    result = [updated_des, updated_details, updated_brand, updated_product_full_name]\n",
    "    result_str = \" \".join(result)\n",
    "    \n",
    "    # Obtain the TF array for the given string\n",
    "    tf = []\n",
    "    result_str = result_str.split(\" \")\n",
    "    ## Calculate TF\n",
    "    for word in result_str:\n",
    "        tf.append(all_list(result_str)[word]/len(result_str))\n",
    "    ## Calculate TFIDF score\n",
    "    idf = pd.read_csv(idf_score).iloc[:, 1:]\n",
    "    tf_idf_score = {}\n",
    "    i = 0\n",
    "    for word in result_str:\n",
    "        if word in idf.columns:\n",
    "            tf_idf_score[word] = tf[i]*idf.loc[0, word]\n",
    "            i += 1\n",
    "        else:\n",
    "            tf_idf_score[word] = 0\n",
    "\n",
    "    ## Get a 1*500 vector for the product\n",
    "    tf_idf = []\n",
    "    i = 0\n",
    "    for col in idf.columns:\n",
    "        if col in result_str:\n",
    "            tf_idf.append(tf_idf_score[col])\n",
    "            i += 1\n",
    "        else:\n",
    "            tf_idf.append(0)\n",
    "            \n",
    "    tf_idf = np.array(tf_idf)\n",
    "    \n",
    "    return(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging_final(product,data1,product_lem_vector):\n",
    "    \"\"\"\n",
    "    Obtain a TFIDF vector for a given product\n",
    "    \n",
    "    Parameters:\n",
    "    arg1(string): The TF-IDF vector of a single product\n",
    "    arg2(dataframe): All the tagged products we have\n",
    "    arg3(array): All the sentence vectors of tagged products\n",
    "    \n",
    "    Return the TFIDF vector\n",
    "    \"\"\"\n",
    "    \n",
    "    import scipy\n",
    "    \n",
    "    product_tag = pd.DataFrame()\n",
    "#    product_tag = pd.Series()\n",
    "#    product_tag = []\n",
    "    for attr in ['occasion', 'style', 'subcategory_bottom', 'subcategory_top']:\n",
    "        similarities = []\n",
    "        df = data1[data1['attribute_name']==attr]\n",
    "        for index in df.index:\n",
    "            item = product_lem_vector[index]\n",
    "            similarity = 1 - scipy.spatial.distance.cosine(product,item)\n",
    "            similarities.append(similarity)\n",
    "        similarities = pd.DataFrame(similarities,index=df.index,columns=['score'])\n",
    "        similarities = similarities[similarities.score<0.999999999]\n",
    "        if attr in ['occasion','style']:\n",
    "            tags = set()\n",
    "            for index in similarities[similarities.score>=t1].index:\n",
    "                tags.add(data1.loc[index,'new_attribute_value'])\n",
    "            product_tag[attr] = [list(tags)]\n",
    "#            for tag in tags:\n",
    "#                product_tag.append([attr,tag])\n",
    "        else:\n",
    "            if similarities.score.max()<t2:\n",
    "                product_tag[attr] = 'N/A'\n",
    "#                product_tag.append([attr,'N/A'])\n",
    "            else:\n",
    "                index = similarities.idxmax()\n",
    "                tag = data1.loc[index,'new_attribute_value'].values\n",
    "                product_tag[attr] = tag\n",
    "#                product_tag.append([attr,tag])\n",
    "#    product_tag = pd.DataFrame(product_tag,columns=['attribute_name','attribute_value'])\n",
    "    return product_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sentence vector\n",
    "product_lem_vector = pd.read_csv('combine_lem_tfidf_score.csv')\n",
    "product_lem_vector.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "product_lem_vector.head()\n",
    "# Turn the panda series into a list of arrays\n",
    "product_lem_vector = np.array(product_lem_vector)\n",
    "product_lem_vector = list(product_lem_vector)\n",
    "# Read data1\n",
    "data1 = pd.read_csv('data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>description</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original Fitness Sneakers</td>\n",
       "      <td>FILA</td>\n",
       "      <td>Vintage Fitness leather sneakers with logo pri...</td>\n",
       "      <td>Leather/synthetic upper\\nLace-up closure\\nText...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAT</td>\n",
       "      <td>CHANEL</td>\n",
       "      <td>unknown</td>\n",
       "      <td>WOOL TWEED &amp; FELT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Petit Oval Buckle Belt</td>\n",
       "      <td>Frame</td>\n",
       "      <td>A Timeless Leather Belt Crafted From Smooth Co...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Little Gir's &amp; Girl's Ariana One-Piece UPF 50+...</td>\n",
       "      <td>Lilly Pulitzer Kids</td>\n",
       "      <td>Pretty ruffle sleeves and trim elevate essenti...</td>\n",
       "      <td>Scoopneck\\nAdjustable straps\\nFlutter sleeves\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baby Girl's Endearing Elephants Pima Cotton Co...</td>\n",
       "      <td>Kissy Kissy</td>\n",
       "      <td>Versatile convertible gown with elephant applique</td>\n",
       "      <td>V-neckline\\nLong sleeves\\nFront snap closure\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name                brand  \\\n",
       "0                          Original Fitness Sneakers                 FILA   \n",
       "1                                                HAT               CHANEL   \n",
       "2                             Petit Oval Buckle Belt                Frame   \n",
       "3  Little Gir's & Girl's Ariana One-Piece UPF 50+...  Lilly Pulitzer Kids   \n",
       "4  Baby Girl's Endearing Elephants Pima Cotton Co...          Kissy Kissy   \n",
       "\n",
       "                                         description  \\\n",
       "0  Vintage Fitness leather sneakers with logo pri...   \n",
       "1                                            unknown   \n",
       "2  A Timeless Leather Belt Crafted From Smooth Co...   \n",
       "3  Pretty ruffle sleeves and trim elevate essenti...   \n",
       "4  Versatile convertible gown with elephant applique   \n",
       "\n",
       "                                             details  \n",
       "0  Leather/synthetic upper\\nLace-up closure\\nText...  \n",
       "1                                  WOOL TWEED & FELT  \n",
       "2                                            unknown  \n",
       "3  Scoopneck\\nAdjustable straps\\nFlutter sleeves\\...  \n",
       "4  V-neckline\\nLong sleeves\\nFront snap closure\\n...  "
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the full data\n",
    "full_data = pd.read_csv('Full data.csv')\n",
    "full_data = full_data.loc[:, [\"name\", \"brand\", \"description\", \"details\"]]\n",
    "full_data.fillna(\"unknown\", inplace = True)\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict 500 sample records on full data\n",
    "t1 = 0.6\n",
    "t2 = 0.6\n",
    "\n",
    "prediction_results = pd.DataFrame()\n",
    "for i in range(500):\n",
    "    prod = full_data.loc[i,:]\n",
    "    description = prod['description']\n",
    "    details = prod['details']\n",
    "    brand = prod['brand']\n",
    "    product_full_name = prod['name']\n",
    "    product = regex_cleaning(description, details, brand, product_full_name, idf_score)\n",
    "    temp = tagging_final(product,data1,product_lem_vector)\n",
    "    prediction_results = pd.concat([prediction_results,temp],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42373, 8)"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data[0:500].copy()\n",
    "full_data[\"occasion\"] = [[]]*len(full_data)\n",
    "full_data[\"style\"] = [[]]*len(full_data)\n",
    "full_data[\"subcategory_bottom\"] = [[]]*len(full_data)\n",
    "full_data[\"subcategory_top\"] = [[]]*len(full_data)\n",
    "\n",
    "full_data[\"occasion\"] = prediction_results[\"occasion\"]\n",
    "full_data[\"style\"] = prediction_results[\"style\"]\n",
    "full_data[\"subcategory_bottom\"] = prediction_results[\"subcategory_bottom\"]\n",
    "full_data[\"subcategory_top\"] = prediction_results[\"subcategory_top\"]\n",
    "\n",
    "full_data.to_csv(\"sample_full_data_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>description</th>\n",
       "      <th>details</th>\n",
       "      <th>occasion</th>\n",
       "      <th>style</th>\n",
       "      <th>subcategory_bottom</th>\n",
       "      <th>subcategory_top</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original Fitness Sneakers</td>\n",
       "      <td>FILA</td>\n",
       "      <td>Vintage Fitness leather sneakers with logo pri...</td>\n",
       "      <td>Leather/synthetic upper\\nLace-up closure\\nText...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAT</td>\n",
       "      <td>CHANEL</td>\n",
       "      <td>unknown</td>\n",
       "      <td>WOOL TWEED &amp; FELT</td>\n",
       "      <td>[Weekend, daytonight, weekend, DaytoNight]</td>\n",
       "      <td>[Classic, classic, casual, Casual]</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Petit Oval Buckle Belt</td>\n",
       "      <td>Frame</td>\n",
       "      <td>A Timeless Leather Belt Crafted From Smooth Co...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>[nightout, daytonight, work, DaytoNight, Weeke...</td>\n",
       "      <td>[glam, Casual, classic, Classic, casual, busin...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Little Gir's &amp; Girl's Ariana One-Piece UPF 50+...</td>\n",
       "      <td>Lilly Pulitzer Kids</td>\n",
       "      <td>Pretty ruffle sleeves and trim elevate essenti...</td>\n",
       "      <td>Scoopneck\\nAdjustable straps\\nFlutter sleeves\\...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baby Girl's Endearing Elephants Pima Cotton Co...</td>\n",
       "      <td>Kissy Kissy</td>\n",
       "      <td>Versatile convertible gown with elephant applique</td>\n",
       "      <td>V-neckline\\nLong sleeves\\nFront snap closure\\n...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name                brand  \\\n",
       "0                          Original Fitness Sneakers                 FILA   \n",
       "1                                                HAT               CHANEL   \n",
       "2                             Petit Oval Buckle Belt                Frame   \n",
       "3  Little Gir's & Girl's Ariana One-Piece UPF 50+...  Lilly Pulitzer Kids   \n",
       "4  Baby Girl's Endearing Elephants Pima Cotton Co...          Kissy Kissy   \n",
       "\n",
       "                                         description  \\\n",
       "0  Vintage Fitness leather sneakers with logo pri...   \n",
       "1                                            unknown   \n",
       "2  A Timeless Leather Belt Crafted From Smooth Co...   \n",
       "3  Pretty ruffle sleeves and trim elevate essenti...   \n",
       "4  Versatile convertible gown with elephant applique   \n",
       "\n",
       "                                             details  \\\n",
       "0  Leather/synthetic upper\\nLace-up closure\\nText...   \n",
       "1                                  WOOL TWEED & FELT   \n",
       "2                                            unknown   \n",
       "3  Scoopneck\\nAdjustable straps\\nFlutter sleeves\\...   \n",
       "4  V-neckline\\nLong sleeves\\nFront snap closure\\n...   \n",
       "\n",
       "                                            occasion  \\\n",
       "0                                                 []   \n",
       "1         [Weekend, daytonight, weekend, DaytoNight]   \n",
       "2  [nightout, daytonight, work, DaytoNight, Weeke...   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                               style subcategory_bottom  \\\n",
       "0                                                 []                N/A   \n",
       "1                 [Classic, classic, casual, Casual]                N/A   \n",
       "2  [glam, Casual, classic, Classic, casual, busin...                N/A   \n",
       "3                                                 []                N/A   \n",
       "4                                                 []                N/A   \n",
       "\n",
       "  subcategory_top  \n",
       "0             N/A  \n",
       "1             N/A  \n",
       "2             N/A  \n",
       "3             N/A  \n",
       "4             N/A  "
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input description: unknown\t\n",
      "Please input details: WOOL TWEED & FELT\n",
      "Please input brand: CHANEL\t\n",
      "Please product full name: HAT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occasion</th>\n",
       "      <th>style</th>\n",
       "      <th>subcategory_bottom</th>\n",
       "      <th>subcategory_top</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Weekend, daytonight, weekend, DaytoNight]</td>\n",
       "      <td>[Classic, classic, casual, Casual]</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     occasion  \\\n",
       "0  [Weekend, daytonight, weekend, DaytoNight]   \n",
       "\n",
       "                                style subcategory_bottom subcategory_top  \n",
       "0  [Classic, classic, casual, Casual]                N/A             N/A  "
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input essential features\n",
    "description = input(\"Please input description: \")\n",
    "details = input(\"Please input details: \")\n",
    "brand = input(\"Please input brand: \")\n",
    "product_full_name = input(\"Please product full name: \")\n",
    "idf_score = \"idf.csv\"\n",
    "\n",
    "result = regex_cleaning(description, details, brand, product_full_name, idf_score)\n",
    "data1 = pd.read_csv(\"data1.csv\")\n",
    "product_lem_vector = pd.read_csv('combine_lem_tfidf_score.csv')\n",
    "product_lem_vector.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "product_lem_vector.head()\n",
    "# Turn the panda series into a list of arrays\n",
    "product_lem_vector = np.array(product_lem_vector)\n",
    "product_lem_vector = list(product_lem_vector)\n",
    "\n",
    "tagging_final(result, data1, product_lem_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
