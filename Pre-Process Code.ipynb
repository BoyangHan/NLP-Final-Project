{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import difflib\n",
    "import spacy\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ballwang\\anaconda3.3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (13,14,15,16,17) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('nlp project.csv', encoding = 'utf-8').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_attribute_value = []\n",
    "for index in data.attribute_value:\n",
    "    index = index.lower()\n",
    "    index = re.sub(r'\\s','', index)\n",
    "    new_attribute_value.append(index)\n",
    "data['new_attribute_value'] = new_attribute_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data\n",
    "data1 = data[['product_id', 'brand','brand_category','description','product_full_name','details','attribute_name', 'new_attribute_value', 'file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ballwang\\anaconda3.3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# deduct duplicate rows\n",
    "data1.drop_duplicates(inplace = True)\n",
    "data1.to_csv(\"data1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69399"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>description</th>\n",
       "      <th>product_full_name</th>\n",
       "      <th>details</th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>new_attribute_value</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01DVPPGPMCZVXSTVDN8E2XSFN3</td>\n",
       "      <td>Albus Lumen</td>\n",
       "      <td>Bags / Tote Bags / Tote Bags</td>\n",
       "      <td>Black leather (Lamb) Open top  Weighs approxim...</td>\n",
       "      <td>Sensillo leather tote</td>\n",
       "      <td>This item's measurements are:\\r\\nDepth 0.5cm\\r...</td>\n",
       "      <td>occasion</td>\n",
       "      <td>daytonight</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01E5ZXWJKJQ1AQFSQY3N1FT5WG</td>\n",
       "      <td>Re/done</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Retro-inspired yet current, this stretchy, rib...</td>\n",
       "      <td>60s Rib Bodysuit</td>\n",
       "      <td>True to size. XS=0-2, S=4-6, M=8-10, L=12.\\r\\n...</td>\n",
       "      <td>occasion</td>\n",
       "      <td>weekend</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01E5ZXWJKJQ1AQFSQY3N1FT5WG</td>\n",
       "      <td>Re/done</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Retro-inspired yet current, this stretchy, rib...</td>\n",
       "      <td>60s Rib Bodysuit</td>\n",
       "      <td>True to size. XS=0-2, S=4-6, M=8-10, L=12.\\r\\n...</td>\n",
       "      <td>occasion</td>\n",
       "      <td>daytonight</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01DVBTEVG1Y5T5DFK6MWKHM13B</td>\n",
       "      <td>ATM Anthony Thomas Melillo</td>\n",
       "      <td>Clothing / Tops / T-Shirts</td>\n",
       "      <td>Black stretch-Pima cotton jersey Slips on 94% ...</td>\n",
       "      <td>Stretch-Pima cotton jersey T-shirt</td>\n",
       "      <td>Fits true to size, take your normal size\\r\\nCu...</td>\n",
       "      <td>occasion</td>\n",
       "      <td>weekend</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01DVPAKV4ANX7BXT77G0F0EART</td>\n",
       "      <td>Sally LaPointe</td>\n",
       "      <td>women:CLOTHING:PANTS</td>\n",
       "      <td>Crafted from supple leather, Sally LaPointe's ...</td>\n",
       "      <td>Belted Leather Wide-Leg  Pants</td>\n",
       "      <td>As seen on the Fall '19 runway\\r\\nRemovable wa...</td>\n",
       "      <td>occasion</td>\n",
       "      <td>weekend</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92025</th>\n",
       "      <td>01DSP0EHJP3KEWQJDC6Q8MTBCS</td>\n",
       "      <td>Movado</td>\n",
       "      <td>JewelryAccessories/Watches/ForHer/Contemporary...</td>\n",
       "      <td>From the Connect Collection. Sleek ion-plated ...</td>\n",
       "      <td>Connect 2.0 Ion-Plated Rose Gold and Black Sta...</td>\n",
       "      <td>Digital Smart module\\r\\nStationary black bezel...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92026</th>\n",
       "      <td>01DSP0EJSPGQVG4X58MDJWDQTP</td>\n",
       "      <td>Ralph Lauren</td>\n",
       "      <td>JustKids/Boys220/ToddlerBoys24/Bottoms,JustKid...</td>\n",
       "      <td>Dip-dyed cotton chino combines with frayed hem...</td>\n",
       "      <td>Little Boy's &amp; Boy's Straight-Fit Dip-Dyed Cot...</td>\n",
       "      <td>Belt loops\\r\\nButton closure\\r\\nZip fly\\r\\nFro...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92027</th>\n",
       "      <td>01DSP0ENYESDKF0FRRV3EP3QJ4</td>\n",
       "      <td>L'Objet</td>\n",
       "      <td>HomeHiTech/HG2019OLDDNU/DiningEntertaining/Din...</td>\n",
       "      <td>From the Alchimie Collection, this bowl has an...</td>\n",
       "      <td>Alchimie Coupe Bowl</td>\n",
       "      <td>Depth, about 12\"\\r\\nEarthenware\\r\\nDishwasher ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92028</th>\n",
       "      <td>01DSP0EQ3R35KP5C7XN4C93YZS</td>\n",
       "      <td>Bonobos</td>\n",
       "      <td>TheMensStore/Apparel/Pants/Dress</td>\n",
       "      <td></td>\n",
       "      <td>Weekday Warrior Slim-Fit Pants</td>\n",
       "      <td>Cotton dress pants that matched your formal we...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92029</th>\n",
       "      <td>01DSP0EWNV2B36FYTT3XMN47H0</td>\n",
       "      <td>Tom Ford</td>\n",
       "      <td>JewelryAccessories/SunglassesReaders/Oversized...</td>\n",
       "      <td>Sleek cat eye sunglasses in a tortoise acetate...</td>\n",
       "      <td>Maxine 56MM Cat Eye Sunglasses</td>\n",
       "      <td>100% UV protection\\r\\nGradient lenses\\r\\nCloth...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69399 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       product_id                       brand  \\\n",
       "0      01DVPPGPMCZVXSTVDN8E2XSFN3                 Albus Lumen   \n",
       "1      01E5ZXWJKJQ1AQFSQY3N1FT5WG                     Re/done   \n",
       "2      01E5ZXWJKJQ1AQFSQY3N1FT5WG                     Re/done   \n",
       "3      01DVBTEVG1Y5T5DFK6MWKHM13B  ATM Anthony Thomas Melillo   \n",
       "4      01DVPAKV4ANX7BXT77G0F0EART              Sally LaPointe   \n",
       "...                           ...                         ...   \n",
       "92025  01DSP0EHJP3KEWQJDC6Q8MTBCS                      Movado   \n",
       "92026  01DSP0EJSPGQVG4X58MDJWDQTP                Ralph Lauren   \n",
       "92027  01DSP0ENYESDKF0FRRV3EP3QJ4                     L'Objet   \n",
       "92028  01DSP0EQ3R35KP5C7XN4C93YZS                     Bonobos   \n",
       "92029  01DSP0EWNV2B36FYTT3XMN47H0                    Tom Ford   \n",
       "\n",
       "                                          brand_category  \\\n",
       "0                           Bags / Tote Bags / Tote Bags   \n",
       "1                                                Unknown   \n",
       "2                                                Unknown   \n",
       "3                             Clothing / Tops / T-Shirts   \n",
       "4                                   women:CLOTHING:PANTS   \n",
       "...                                                  ...   \n",
       "92025  JewelryAccessories/Watches/ForHer/Contemporary...   \n",
       "92026  JustKids/Boys220/ToddlerBoys24/Bottoms,JustKid...   \n",
       "92027  HomeHiTech/HG2019OLDDNU/DiningEntertaining/Din...   \n",
       "92028                   TheMensStore/Apparel/Pants/Dress   \n",
       "92029  JewelryAccessories/SunglassesReaders/Oversized...   \n",
       "\n",
       "                                             description  \\\n",
       "0      Black leather (Lamb) Open top  Weighs approxim...   \n",
       "1      Retro-inspired yet current, this stretchy, rib...   \n",
       "2      Retro-inspired yet current, this stretchy, rib...   \n",
       "3      Black stretch-Pima cotton jersey Slips on 94% ...   \n",
       "4      Crafted from supple leather, Sally LaPointe's ...   \n",
       "...                                                  ...   \n",
       "92025  From the Connect Collection. Sleek ion-plated ...   \n",
       "92026  Dip-dyed cotton chino combines with frayed hem...   \n",
       "92027  From the Alchimie Collection, this bowl has an...   \n",
       "92028                                                      \n",
       "92029  Sleek cat eye sunglasses in a tortoise acetate...   \n",
       "\n",
       "                                       product_full_name  \\\n",
       "0                                  Sensillo leather tote   \n",
       "1                                       60s Rib Bodysuit   \n",
       "2                                       60s Rib Bodysuit   \n",
       "3                     Stretch-Pima cotton jersey T-shirt   \n",
       "4                         Belted Leather Wide-Leg  Pants   \n",
       "...                                                  ...   \n",
       "92025  Connect 2.0 Ion-Plated Rose Gold and Black Sta...   \n",
       "92026  Little Boy's & Boy's Straight-Fit Dip-Dyed Cot...   \n",
       "92027                                Alchimie Coupe Bowl   \n",
       "92028                     Weekday Warrior Slim-Fit Pants   \n",
       "92029                     Maxine 56MM Cat Eye Sunglasses   \n",
       "\n",
       "                                                 details attribute_name  \\\n",
       "0      This item's measurements are:\\r\\nDepth 0.5cm\\r...       occasion   \n",
       "1      True to size. XS=0-2, S=4-6, M=8-10, L=12.\\r\\n...       occasion   \n",
       "2      True to size. XS=0-2, S=4-6, M=8-10, L=12.\\r\\n...       occasion   \n",
       "3      Fits true to size, take your normal size\\r\\nCu...       occasion   \n",
       "4      As seen on the Fall '19 runway\\r\\nRemovable wa...       occasion   \n",
       "...                                                  ...            ...   \n",
       "92025  Digital Smart module\\r\\nStationary black bezel...                  \n",
       "92026  Belt loops\\r\\nButton closure\\r\\nZip fly\\r\\nFro...                  \n",
       "92027  Depth, about 12\"\\r\\nEarthenware\\r\\nDishwasher ...                  \n",
       "92028  Cotton dress pants that matched your formal we...                  \n",
       "92029  100% UV protection\\r\\nGradient lenses\\r\\nCloth...                  \n",
       "\n",
       "      new_attribute_value        file  \n",
       "0              daytonight  additional  \n",
       "1                 weekend  additional  \n",
       "2              daytonight  additional  \n",
       "3                 weekend  additional  \n",
       "4                 weekend  additional  \n",
       "...                   ...         ...  \n",
       "92025                                  \n",
       "92026                                  \n",
       "92027                                  \n",
       "92028                                  \n",
       "92029                                  \n",
       "\n",
       "[69399 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description Feature Pre-Processing (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nosymbol_description = []\n",
    "# find out all the percentage material in descriptions\n",
    "for sent in data1.description:\n",
    "    nosymbol_description.append(re.sub(r'[^\\w+0-9%.]', ' ', sent, flags = re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this create substitute terms based on the percentage material\n",
    "all_m = pd.Series(nosymbol_description).str.findall(r'(\\d+)(?:%)\\s(\\w+)')\n",
    "material = []\n",
    "temp_content = ''\n",
    "temp = []\n",
    "number = 0\n",
    "for i in range(len(all_m)):\n",
    "    if all_m[i] != []:\n",
    "        for j in all_m[i]:\n",
    "            for k in j:\n",
    "                if k == j[0]:\n",
    "                    number = int(k)\n",
    "                    if number >= 80:\n",
    "                        temp_content = temp_content + \"HIGH_\"\n",
    "                    elif number >= 40:\n",
    "                        temp_content = temp_content + \"MEDIAN_\"\n",
    "                    else:\n",
    "                        temp_content = temp_content + \"LOW_\"\n",
    "                if k == j[-1]:\n",
    "                    temp_content = temp_content + k.upper()\n",
    "            \n",
    "            temp.append(temp_content)\n",
    "            temp_content = ''\n",
    "        material.append(temp)\n",
    "        temp = []\n",
    "    else:\n",
    "         material.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all the percentage content as MATERIAL\n",
    "material_description = []\n",
    "for i in data1.description:\n",
    "    material_description.append(re.sub(r'(\\d+)(?:%)\\s(\\w+)', 'MATERIAL', i, flags = re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find out where the MATERIAL terms locate in each document and replace them with substitute terms created earlier\n",
    "count = 0\n",
    "cleaned_material = []\n",
    "for i in range(len(material_description)):\n",
    "    new_word = []\n",
    "    for j in material_description[i].split(' '):\n",
    "        if j == 'MATERIAL':\n",
    "            new_word.append(material[i][count])\n",
    "            count += 1\n",
    "        else:\n",
    "            new_word.append(j)\n",
    "    count = 0\n",
    "    clean = \" \".join(new_word)\n",
    "    cleaned_material.append(clean)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_numeric_description = []\n",
    "for i in cleaned_material:\n",
    "    no_numeric_description.append(re.sub(r'((\\d+\\.\\d+)|(\\d+))', 'NUMERICAL_VALUE ', i, flags = re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(lines, delimiter = \" \"):\n",
    "    words = Counter()\n",
    "    for line in lines:\n",
    "        line = line.lower()\n",
    "        for word in line.split(delimiter):\n",
    "            words[word] += 1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = word_count(no_numeric_description)\n",
    "count_sort = sorted((value, key) for (key, value) in count.items())[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " '\\r\\n',\n",
       " '\\r\\n\\r\\nbody:',\n",
       " '\"',\n",
       " '\")',\n",
       " '&',\n",
       " \"'\",\n",
       " \"'numerical_value\",\n",
       " '(numerical_value',\n",
       " ',',\n",
       " '-',\n",
       " '/numerical_value',\n",
       " 'across',\n",
       " 'add',\n",
       " 'added',\n",
       " 'adds',\n",
       " 'adjustable',\n",
       " 'allover',\n",
       " 'along',\n",
       " 'also',\n",
       " 'ankle',\n",
       " 'approximately',\n",
       " 'around',\n",
       " 'available',\n",
       " 'back',\n",
       " 'back.',\n",
       " 'bag',\n",
       " 'belt',\n",
       " 'best',\n",
       " 'black',\n",
       " 'blazer',\n",
       " 'blend',\n",
       " 'blouse',\n",
       " 'blue',\n",
       " 'body',\n",
       " 'bold',\n",
       " 'boot',\n",
       " 'boots',\n",
       " \"brand's\",\n",
       " 'breathable',\n",
       " 'bring',\n",
       " 'brings',\n",
       " 'button',\n",
       " 'button-down',\n",
       " 'buttons',\n",
       " 'care',\n",
       " 'cashmere',\n",
       " 'casual',\n",
       " 'center',\n",
       " 'chest',\n",
       " 'chic',\n",
       " 'chunky',\n",
       " 'classic',\n",
       " 'clean',\n",
       " 'closure',\n",
       " 'closure.',\n",
       " 'cm',\n",
       " 'coat',\n",
       " 'collar',\n",
       " 'collar.',\n",
       " 'collection',\n",
       " 'collection.',\n",
       " 'color',\n",
       " 'color:',\n",
       " 'colors',\n",
       " 'comes',\n",
       " 'comfort',\n",
       " 'comfort.',\n",
       " 'comfortable',\n",
       " 'comfy',\n",
       " 'complete',\n",
       " 'concealed',\n",
       " 'construction',\n",
       " 'contrast',\n",
       " 'contrasting',\n",
       " 'cool',\n",
       " 'cotton',\n",
       " 'cotton-blend',\n",
       " 'cotton.',\n",
       " 'cozy',\n",
       " 'crafted',\n",
       " 'create',\n",
       " 'crepe',\n",
       " 'crew',\n",
       " 'crewneck',\n",
       " 'crisp',\n",
       " 'cropped',\n",
       " 'crossbody',\n",
       " 'cuffs',\n",
       " 'cuffs.',\n",
       " 'cut',\n",
       " 'dark',\n",
       " 'day',\n",
       " 'delicate',\n",
       " 'denim',\n",
       " 'denim,',\n",
       " 'depth:\\r\\n',\n",
       " 'design',\n",
       " 'design,',\n",
       " 'design.',\n",
       " 'designed',\n",
       " 'designer',\n",
       " 'detail',\n",
       " 'detail.',\n",
       " 'detailed',\n",
       " 'detailing',\n",
       " 'details',\n",
       " 'double',\n",
       " 'drawstring',\n",
       " 'dress',\n",
       " 'drop:\\r\\n',\n",
       " 'dry',\n",
       " 'easy',\n",
       " 'edge',\n",
       " 'effortless',\n",
       " 'elastic',\n",
       " 'elegant',\n",
       " 'elevated',\n",
       " 'embroidered',\n",
       " 'engineered',\n",
       " 'enough',\n",
       " 'essential',\n",
       " 'even',\n",
       " 'every',\n",
       " 'everyday',\n",
       " 'everything',\n",
       " 'extra',\n",
       " 'eye',\n",
       " 'fabric',\n",
       " 'fall',\n",
       " 'fastening',\n",
       " 'fastenings',\n",
       " 'faux',\n",
       " 'favorite',\n",
       " 'feature',\n",
       " 'features',\n",
       " 'featuring',\n",
       " 'feel',\n",
       " 'feel.',\n",
       " 'feminine',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'finish.',\n",
       " 'finished',\n",
       " 'first',\n",
       " 'fit',\n",
       " 'fit,',\n",
       " 'fit.',\n",
       " 'fitted',\n",
       " 'flap',\n",
       " 'flat',\n",
       " 'flattering',\n",
       " 'floral',\n",
       " 'fly',\n",
       " 'formula',\n",
       " 'frames',\n",
       " 'fresh',\n",
       " 'front',\n",
       " 'front.',\n",
       " 'full',\n",
       " 'fully',\n",
       " 'fun',\n",
       " 'get',\n",
       " 'give',\n",
       " 'gives',\n",
       " 'gold',\n",
       " 'graphic',\n",
       " 'great',\n",
       " 'hand',\n",
       " 'handle',\n",
       " 'heel',\n",
       " 'heel.',\n",
       " 'height:\\r\\n',\n",
       " 'hem',\n",
       " 'hem.',\n",
       " 'high',\n",
       " 'high-rise',\n",
       " 'high-waisted',\n",
       " 'high_cotton',\n",
       " 'high_polyester',\n",
       " 'hips',\n",
       " 'hoodie',\n",
       " 'iconic',\n",
       " 'ideal',\n",
       " 'imported',\n",
       " 'in,',\n",
       " 'in;',\n",
       " 'inches',\n",
       " 'inspired',\n",
       " 'instant',\n",
       " 'interior',\n",
       " 'italian',\n",
       " 'italy',\n",
       " 'it’s',\n",
       " 'jacket',\n",
       " 'japanese',\n",
       " 'jean',\n",
       " 'jeans',\n",
       " 'jersey',\n",
       " 'keep',\n",
       " 'knit',\n",
       " 'lace',\n",
       " 'layer',\n",
       " 'leather',\n",
       " 'leather,',\n",
       " 'leather.',\n",
       " 'leg',\n",
       " 'lend',\n",
       " 'lends',\n",
       " 'length',\n",
       " 'length:',\n",
       " 'light',\n",
       " 'lightweight',\n",
       " 'like',\n",
       " 'lined',\n",
       " 'linen',\n",
       " 'lining:',\n",
       " 'little',\n",
       " 'logo',\n",
       " 'long',\n",
       " 'look',\n",
       " 'look.',\n",
       " 'looks',\n",
       " 'love',\n",
       " 'luxe',\n",
       " 'luxurious',\n",
       " 'machine',\n",
       " 'made',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'matching',\n",
       " 'material,',\n",
       " 'material:\\r\\n',\n",
       " 'material;',\n",
       " 'measurements:\\r\\n',\n",
       " 'measures',\n",
       " 'median_polyester',\n",
       " 'medium',\n",
       " 'mesh',\n",
       " 'metal',\n",
       " 'metallic',\n",
       " 'midi',\n",
       " 'mini',\n",
       " 'minimalist',\n",
       " 'mm/',\n",
       " 'model',\n",
       " 'modern',\n",
       " 'natural',\n",
       " 'neck',\n",
       " 'neck.',\n",
       " 'neckline',\n",
       " 'need',\n",
       " 'never',\n",
       " 'new',\n",
       " 'numerical_value',\n",
       " 'offers',\n",
       " 'one',\n",
       " 'open',\n",
       " 'oversized',\n",
       " 'oz.',\n",
       " 'pair',\n",
       " 'pant',\n",
       " 'pants',\n",
       " 'patch',\n",
       " 'pattern',\n",
       " 'perfect',\n",
       " 'perfectly',\n",
       " 'piece',\n",
       " 'pieces',\n",
       " 'pima',\n",
       " 'plaid',\n",
       " 'playful',\n",
       " 'plus,',\n",
       " 'plush',\n",
       " 'pocket',\n",
       " 'pockets',\n",
       " 'pockets,',\n",
       " 'pockets.',\n",
       " 'polished',\n",
       " 'polo',\n",
       " 'premium',\n",
       " 'pretty',\n",
       " 'print',\n",
       " 'print.',\n",
       " 'printed',\n",
       " 'pullover',\n",
       " 'refined',\n",
       " 'relaxed',\n",
       " 'ribbed',\n",
       " 'rich',\n",
       " 'right',\n",
       " 'rise',\n",
       " 'rise,',\n",
       " 'rubber',\n",
       " 'ruffled',\n",
       " 's.',\n",
       " 'saks.',\n",
       " 'sandal',\n",
       " 'sandals',\n",
       " 'satin',\n",
       " 'scarf',\n",
       " 'seams',\n",
       " 'set',\n",
       " 'shape',\n",
       " 'shirt',\n",
       " 'shirttail',\n",
       " 'shoe',\n",
       " 'short',\n",
       " 'shorts',\n",
       " 'shoulder',\n",
       " 'side',\n",
       " 'signature',\n",
       " 'silhouette',\n",
       " 'silhouette,',\n",
       " 'silhouette.',\n",
       " 'silk',\n",
       " 'silver',\n",
       " 'simple',\n",
       " 'since',\n",
       " 'size',\n",
       " 'skin',\n",
       " 'skinny',\n",
       " 'skirt',\n",
       " 'sleek',\n",
       " 'sleeve',\n",
       " 'sleeves',\n",
       " 'sleeves,',\n",
       " 'sleeves.',\n",
       " 'slightly',\n",
       " 'slim',\n",
       " 'slip',\n",
       " 'slips',\n",
       " 'small',\n",
       " 'smooth',\n",
       " 'snap',\n",
       " 'sneaker',\n",
       " 'sneakers',\n",
       " 'soft',\n",
       " 'soft,',\n",
       " 'sole',\n",
       " 'sophisticated',\n",
       " 'special',\n",
       " 'specially',\n",
       " 'sporty',\n",
       " 'square',\n",
       " 'staple',\n",
       " 'statement',\n",
       " 'steel',\n",
       " 'straight',\n",
       " 'strap',\n",
       " 'straps',\n",
       " 'stretch',\n",
       " 'striped',\n",
       " 'stripes',\n",
       " 'structured',\n",
       " 'style',\n",
       " 'style.',\n",
       " 'styled',\n",
       " 'styling',\n",
       " 'styling.',\n",
       " 'stylish',\n",
       " 'subtle',\n",
       " 'suede',\n",
       " 'suit',\n",
       " 'summer',\n",
       " 'sunglasses',\n",
       " 'super',\n",
       " 'sweater',\n",
       " 'sweatshirt',\n",
       " 't-shirt',\n",
       " 'tailored',\n",
       " 'take',\n",
       " 'tall',\n",
       " 'tapered',\n",
       " 'tee',\n",
       " 'texture',\n",
       " 'textured',\n",
       " 'thanks',\n",
       " \"that's\",\n",
       " \"they're\",\n",
       " 'tie',\n",
       " 'timeless',\n",
       " 'toe',\n",
       " 'tonal',\n",
       " 'top',\n",
       " 'touch',\n",
       " 'traditional',\n",
       " 'trim',\n",
       " 'trim.',\n",
       " 'trousers',\n",
       " 'true',\n",
       " 'twill',\n",
       " 'two',\n",
       " 'ultimate',\n",
       " 'unique',\n",
       " 'updated',\n",
       " 'use',\n",
       " 'used',\n",
       " 'vegan',\n",
       " 'velvet',\n",
       " 'versatile',\n",
       " 'version',\n",
       " 'vibrant',\n",
       " 'vintage',\n",
       " 'waist',\n",
       " 'waist,',\n",
       " 'waist.',\n",
       " 'waistband',\n",
       " 'wardrobe',\n",
       " 'warm',\n",
       " 'wash',\n",
       " 'wash.',\n",
       " 'washed',\n",
       " 'water',\n",
       " 'way',\n",
       " 'wear',\n",
       " 'wearing',\n",
       " 'wears',\n",
       " 'well',\n",
       " 'welt',\n",
       " 'white',\n",
       " 'wide',\n",
       " 'wide-leg',\n",
       " 'width:\\r\\n',\n",
       " 'without',\n",
       " 'wool',\n",
       " 'worn',\n",
       " 'woven',\n",
       " 'wrap',\n",
       " 'x',\n",
       " 'yet',\n",
       " 'zip',\n",
       " 'zipper',\n",
       " '”'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out the difference between the 'stopwords' list and most frequent words\n",
    "stopword = set(stopwords.words('english'))\n",
    "count_sorted_set = set(pd.DataFrame(count_sort[:500])[1])\n",
    "count_sorted_set.difference_update(stopword)\n",
    "count_sorted_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the 'stopwords' list\n",
    "updated_stopwords = stopwords.words('english') + [\n",
    "    '.' , '', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_descriptions = []\n",
    "updated_stopwords = set(updated_stopwords)\n",
    "\n",
    "for line in no_numeric_description:\n",
    "    new_words = []\n",
    "    line = re.findall(r'\\b[a-zA-Z0-9_]{2,}\\b',line)\n",
    "    for word in line:\n",
    "        if word.lower() in updated_stopwords:\n",
    "            continue\n",
    "        new_words.append(word.lower())\n",
    "    cleaned_description = \" \".join(new_words)\n",
    "    cleaned_descriptions.append(cleaned_description)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brand Feature Pre-Processing (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ballwang\\anaconda3.3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\ballwang\\anaconda3.3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\ballwang\\anaconda3.3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\ballwang\\anaconda3.3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "def removeSpecialChars(z):\n",
    "    \n",
    "    return z.translate ({ord(c): \"\" for c in \"!@#$%^&*()[]{};:,./'<>?\\|`~-=_+\"})\n",
    "\n",
    "def initial_clean(data,column):\n",
    "    \n",
    "    data[column]=data[column].apply(str)\n",
    "    data[column]=data[column].apply(lambda x:x.lower())\n",
    "    data[column]=data[column].apply(lambda x:re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x))\n",
    "    data[column]=data[column].apply(lambda x:removeSpecialChars(x))\n",
    "    \n",
    "initial_clean(data1,'brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = data1['brand'].fillna(\" \")\n",
    "stopwords = ['by','and','the','of','for','to','inc','','a','an']\n",
    "def remove_stopwords(text: str):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = list(filter(lambda token: token not in stopwords, tokens))\n",
    "    return \" \".join(filtered_tokens)\n",
    "brand = brand.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       albus lumen\n",
       "1                            redone\n",
       "2                            redone\n",
       "3        atm anthony thomas melillo\n",
       "4                    sally lapointe\n",
       "                    ...            \n",
       "92025                        movado\n",
       "92026                  ralph lauren\n",
       "92027                        lobjet\n",
       "92028                       bonobos\n",
       "92029                      tom ford\n",
       "Name: brand, Length: 69399, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details Feature Pre-Processsing (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get required features and fill in null values with blank\n",
    "df1=data1[\"details\"]\n",
    "df1=df1.fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        this items measurements are depth cm height cm...\n",
       "1        true to size xs s m l snap closure crewneck lo...\n",
       "2        true to size xs s m l snap closure crewneck lo...\n",
       "3        fits true to size take your normal size cut fo...\n",
       "4        as seen on the fall  runway removable waist un...\n",
       "                               ...                        \n",
       "92025    digital smart module stationary black bezel go...\n",
       "92026    belt loops button closure zip fly front angled...\n",
       "92027    depth about  earthenware dishwasher safe made ...\n",
       "92028    cotton dress pants that matched your formal we...\n",
       "92029     uv protection gradient lenses cloth and case ...\n",
       "Name: details, Length: 69399, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove punctuations and numbers\n",
    "df1=df1.apply(lambda x: x.lower()).str.replace(r\"[^a-z\\s]+\", \"\", case=False)\n",
    "df1=df1.str.replace(r\"\\r\\n\", \" \", case=False)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=word_count(list(df1))\n",
    "count_sort=sorted((value, key) for (key, value) in count.items())[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ballwang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "# set(stopwords.words(\"English\"))\n",
    "nltk_stopwords = list(set(stopwords.words(\"English\")))\n",
    "count_sorted_set = set(pd.DataFrame(count_sort[:100])[1])\n",
    "count_sorted_set.difference_update(nltk_stopwords)\n",
    "nltk_stopwords.extend([\"\",\"h\",\"l\",\"mm\",\"w\",\"x\",\"xl\",\"xs\",\"cm\",\"uv\"])\n",
    "\n",
    "def remove_stopwords(title: str):\n",
    "    tokens = nltk.word_tokenize(title)\n",
    "    filtered_tokens = list(filter(lambda token: token not in nltk_stopwords, tokens))\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    items measurements depth height width\n",
       "1        true size snap closure crewneck long sleeves m...\n",
       "2        true size snap closure crewneck long sleeves m...\n",
       "3        fits true size take normal size cut slim fit l...\n",
       "4        seen fall runway removable waist unlined butto...\n",
       "                               ...                        \n",
       "92025    digital smart module stationary black bezel go...\n",
       "92026    belt loops button closure zip fly front angled...\n",
       "92027      depth earthenware dishwasher safe made portugal\n",
       "92028    cotton dress pants matched formal wear belt lo...\n",
       "92029    protection gradient lenses cloth case included...\n",
       "Name: details, Length: 69399, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df1.apply(lambda x: remove_stopwords(x))\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Name Pre-Processing (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make everything lowercase\n",
    "product_full_name = data1['product_full_name'].str.lower()\n",
    "# Remove hyphen\n",
    "product_full_name = product_full_name.str.replace(\"-\",\"\",case=False)\n",
    "# Remove numbers\n",
    "product_full_name = product_full_name.str.replace(r'[0-9]+',\"\",case=False)\n",
    "# Remove special characters and stop words\n",
    "import nltk\n",
    "stopwords = ['&','|',',','.',\"'\",'in','le','mm','and','with','the','k','x','les','de','of',\"'s\"]\n",
    "def remove_stopwords(text: str):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = list(filter(lambda token: token not in stopwords, tokens))\n",
    "    return \" \".join(filtered_tokens)\n",
    "product_full_name = product_full_name.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#function to convert nltk tag to wordnet tag\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "def lemmatize_sentence(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    wn_tagged = map(lambda x :(x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "    res_words = []\n",
    "    for word, tag in wn_tagged:\n",
    "        if tag is None:\n",
    "            res_words.append(word)\n",
    "        else:\n",
    "            res_words.append(lemmatizer.lemmatize(word,tag))\n",
    "    return \" \".join(res_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_description_lem = []\n",
    "for sentence in cleaned_descriptions:\n",
    "    cleaned_description_lem = cleaned_description_lem + [lemmatize_sentence(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_brand_lem = []\n",
    "for sentence in list(brand):\n",
    "    cleaned_brand_lem = cleaned_brand_lem + [lemmatize_sentence(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_details_lem = []\n",
    "for sentence in list(df1):\n",
    "    cleaned_details_lem = cleaned_details_lem + [lemmatize_sentence(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_productname_lem = []\n",
    "for sentence in list(product_full_name):\n",
    "    cleaned_productname_lem = cleaned_productname_lem + [lemmatize_sentence(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Score (Combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to combine each lemmatizating feature first. Then, we computed TF-IDF scores based on the combining document file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_lem = pd.Series(cleaned_description_lem)+\" \"+pd.Series(cleaned_brand_lem)+\" \"+\\\n",
    "        pd.Series(cleaned_details_lem)+\" \"+pd.Series(cleaned_productname_lem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,1),\n",
    "                             token_pattern=r'\\b[a-zA-Z0-9]{2,}\\b',\n",
    "                             max_df=0.8,\n",
    "                             min_df = 0.01, stop_words=updated_stopwords, max_features = 500, use_idf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(combine_lem)\n",
    "terms = vectorizer.get_feature_names()\n",
    "combine_lem_tfidf_score = pd.DataFrame(X.toarray(), columns=terms)\n",
    "combine_lem_idf_score = vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of figuring out IDF score is for new input data to figure out TF-IDF score\n",
    "combine_lem_idf_score = pd.DataFrame(combine_lem_idf_score)\n",
    "combine_lem_idf_score.rename(index = {0:'idf_score'}, inplace = True)\n",
    "combine_lem_idf_score.rename(columns = dict(zip(combine_lem_idf_score.columns,combine_lem_tfidf_score.columns)), inplace = True)\n",
    "combine_lem_tfidf_score = combine_lem_tfidf_score.append(combine_lem_idf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_lem_tfidf_score.to_csv('combine_lem_tfidf_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_lem_idf_score.to_csv('idf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_lem_tfidf_score.to_csv('combine_lem_tfidf_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_stemm_tfidf_score.to_csv('combine_stemm_tfidf_score.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (Combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We computed the Word2Vec by using combined lemmatization document file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(sentence) for sentence in combine_lem ]\n",
    "combine_lem_word2vec_model = Word2Vec(docs, min_count = 1, window = 5, size = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_lem_word2vec_model.save('combine_lem_word2vec_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lem Weighting Scheme (TF-IDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(combine_lem)\n",
    "tf_idf_lookup_table = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_SUM_COLUMN = \"DOCUMENT_TF_IDF_SUM\"\n",
    "\n",
    "tf_idf_lookup_table[DOCUMENT_SUM_COLUMN] = tf_idf_lookup_table.sum(axis=1)\n",
    "available_tf_idf_scores = tf_idf_lookup_table.columns # a list of all the columns we have\n",
    "available_tf_idf_scores = list(map(lambda x: x.lower(), available_tf_idf_scores)) # lowercase everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ballwang\\anaconda3.3\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# load spacy en_core_web_md model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "weighted_vectors = []\n",
    "for idx, item in enumerate(combine_lem): # iterate through each product\n",
    " \n",
    "    tokens = nlp(item) # have spacy tokenize the product text\n",
    "    # initially start a running total of tf-idf scores for a document\n",
    "    total_tf_idf_score_per_document = 0\n",
    "    \n",
    "    # start a running total of initially all zeroes (300 is picked since that is the word embedding size used by word2vec)\n",
    "    running_total_word_embedding = np.zeros(300) \n",
    "    for token in tokens: # iterate through each token\n",
    "    # if the token has a pretrained word embedding it also has a tf-idf score\n",
    "        if token.has_vector and token.text.lower() in available_tf_idf_scores:\n",
    "            tf_idf_score = tf_idf_lookup_table.loc[idx, token.text.lower()]\n",
    "            #print(f\"{token} has tf-idf score of {tf_idf_lookup_table.loc[idx, token.text.lower()]}\")\n",
    "            running_total_word_embedding += tf_idf_score * token.vector\n",
    "            total_tf_idf_score_per_document += tf_idf_score\n",
    "    \n",
    "    # divide the total embedding by the total tf-idf score for each document\n",
    "    document_embedding = running_total_word_embedding / total_tf_idf_score_per_document\n",
    "    weighted_vectors.append(document_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(np.array(weighted_vectors).reshape(68326, 300)).to_csv('weighted vector.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Vector (Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_lem = list(combine_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(sentence) for sentence in combine_lem]\n",
    "model = Word2Vec(docs, min_count = 1, window = 5, size = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_vector = []\n",
    "for item in combine_lem:\n",
    "    temp_vector = np.zeros((1,500))\n",
    "    n = 0\n",
    "    for word in item.split(\" \"):\n",
    "        if word in model.wv.vocab:\n",
    "            temp_vector += model.wv.get_vector(word)\n",
    "        else:\n",
    "            temp_vector += np.random.normal(size=500)\n",
    "        n += 1\n",
    "    product_vector.append(temp_vector/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(np.array(product_vector).reshape(68326, 500)).to_csv('sentece vector.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
