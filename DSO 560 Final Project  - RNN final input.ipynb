{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import difflib\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('nlp project.csv', encoding = 'utf-8').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_attribute_value = []\n",
    "for index in data.attribute_value:\n",
    "    index = re.sub(r'\\s','', index)\n",
    "    new_attribute_value.append(index)\n",
    "data['new_attribute_value'] = new_attribute_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data[['brand','brand_category','description','product_full_name','details','attribute_name', 'new_attribute_value', 'file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68326"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1=data1.drop_duplicates()\n",
    "len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description Feature Pre-Processing (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "nosymbol_description = []\n",
    "for sent in data.description:\n",
    "    nosymbol_description.append(re.sub(r'[^\\w+0-9%.]', ' ', sent, flags = re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_m = pd.Series(nosymbol_description).str.findall(r'(\\d+)(?:%)\\s(\\w+)')\n",
    "material = []\n",
    "temp_content = ''\n",
    "temp = []\n",
    "number = 0\n",
    "for i in range(len(all_m)):\n",
    "    if all_m[i] != []:\n",
    "        for j in all_m[i]:\n",
    "            for k in j:\n",
    "                if k == j[0]:\n",
    "                    number = int(k)\n",
    "                    if number >= 80:\n",
    "                        temp_content = temp_content + \"HIGH_\"\n",
    "                    elif number >= 40:\n",
    "                        temp_content = temp_content + \"MEDIAN_\"\n",
    "                    else:\n",
    "                        temp_content = temp_content + \"LOW_\"\n",
    "                if k == j[-1]:\n",
    "                    temp_content = temp_content + k.upper()\n",
    "            \n",
    "            temp.append(temp_content)\n",
    "            temp_content = ''\n",
    "        material.append(temp)\n",
    "        temp = []\n",
    "    else:\n",
    "         material.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HIGH_PIMA', 'LOW_ELASTANE']"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "material[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_description = []\n",
    "for i in data.description:\n",
    "    material_description.append(re.sub(r'(\\d+)(?:%)\\s(\\w+)', 'MATERIAL', i, flags = re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "cleaned_material = []\n",
    "for i in range(len(material_description)):\n",
    "    new_word = []\n",
    "    for j in material_description[i].split(' '):\n",
    "        if j == 'MATERIAL':\n",
    "            new_word.append(material[i][count])\n",
    "            count += 1\n",
    "        else:\n",
    "            new_word.append(j.lower())\n",
    "    count = 0\n",
    "    clean = \" \".join(new_word)\n",
    "    cleaned_material.append(clean)\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaned_material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_numeric_description = []\n",
    "for i in cleaned_material:\n",
    "    no_numeric_description.append(re.sub(r'((\\d+\\.\\d+)|(\\d+))', 'NUMERICAL_VALUE ', i, flags = re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no_numeric_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(lines, delimiter = \" \"):\n",
    "    words = Counter()\n",
    "    for line in lines:\n",
    "        line = line.lower()\n",
    "        for word in line.split(delimiter):\n",
    "            words[word] += 1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = word_count(no_numeric_description)\n",
    "count_sort = sorted((value, key) for (key, value) in count.items())[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopword = set(stopwords.words('english'))\n",
    "count_sorted_set = set(pd.DataFrame(count_sort[:500])[1])\n",
    "count_sorted_set.difference_update(stopword)\n",
    "count_sorted_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_stopwords = stopwords.words('english') + [\n",
    "    '.' , '', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_numeric_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_descriptions = []\n",
    "updated_stopwords = set(updated_stopwords)\n",
    "\n",
    "for line in no_numeric_description:\n",
    "    new_words = []\n",
    "    line = re.findall(r'\\b[a-zA-Z0-9_]{2,}\\b',line)\n",
    "    for word in line:\n",
    "        if word.lower() in updated_stopwords:\n",
    "            continue\n",
    "        new_words.append(word)\n",
    "    cleaned_description = \" \".join(new_words)\n",
    "    cleaned_descriptions.append(cleaned_description)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brand Feature Pre-Processing (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpecialChars(z):\n",
    "    \n",
    "    return z.translate ({ord(c): \"\" for c in \"!@#$%^&*()[]{};:,./'<>?\\|`~-=_+\"})\n",
    "\n",
    "def initial_clean(data,column):\n",
    "    \n",
    "    data[column]=data[column].apply(str)\n",
    "    data[column]=data[column].apply(lambda x:x.lower())\n",
    "    data[column]=data[column].apply(lambda x:re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x))\n",
    "    data[column]=data[column].apply(lambda x:removeSpecialChars(x))\n",
    "    \n",
    "initial_clean(data1,'brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = data1['brand'].fillna(\" \")\n",
    "stopwords = ['by','and','the','of','for','to','inc','','a','an']\n",
    "def remove_stopwords(text: str):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = list(filter(lambda token: token not in stopwords, tokens))\n",
    "    return \" \".join(filtered_tokens)\n",
    "brand = brand.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details Feature Pre-Processsing (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get required features and fill in null values with blank\n",
    "df1=data1[\"details\"]\n",
    "df1=df1.fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#extract all % number in details as a new column\n",
    "perc=df1.str.findall(r\"(\\d+)(?:%)\\s(\\w+)\")\n",
    "result=[]\n",
    "temp_result=[]\n",
    "temp=\"\"\n",
    "for i in range(len(perc)):\n",
    "    if perc[i]!=[]:\n",
    "        for j in range(len(perc[i])):\n",
    "            if int(perc[i][j][0])>=80:\n",
    "                temp=\"High_\"+perc[i][j][1]\n",
    "            elif int(perc[i][j][0])>=60:\n",
    "                temp=\"Medium_\"+perc[i][j][1]\n",
    "            else:\n",
    "                temp=\"Low_\"+perc[i][j][1]\n",
    "            temp_result.append(temp)\n",
    "        result.append(temp_result)\n",
    "        temp=\"\"\n",
    "        temp_result=[]\n",
    "    else:\n",
    "        result.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations and numbers\n",
    "df1=df1.apply(lambda x: x.lower()).str.replace(r\"[^a-z\\s]+\", \"\", case=False)\n",
    "df1=df1.str.replace(r\"\\r\\n\", \" \", case=False)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=word_count(list(df1))\n",
    "count_sort=sorted((value, key) for (key, value) in count.items())[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sharon\n",
      "[nltk_data]     Jia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "# set(stopwords.words(\"English\"))\n",
    "nltk_stopwords = list(set(stopwords.words(\"English\")))\n",
    "count_sorted_set = set(pd.DataFrame(count_sort[:100])[1])\n",
    "count_sorted_set.difference_update(nltk_stopwords)\n",
    "nltk_stopwords.extend([\"\",\"h\",\"l\",\"mm\",\"w\",\"x\",\"xl\",\"xs\",\"cm\",\"uv\"])\n",
    "\n",
    "def remove_stopwords(title: str):\n",
    "    tokens = nltk.word_tokenize(title)\n",
    "    filtered_tokens = list(filter(lambda token: token not in nltk_stopwords, tokens))\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check spelling and remove stopwords\n",
    "#df1[\"details\"]=df1[\"details\"].apply(lambda x: spellcheck_document(x))\n",
    "df1=df1.apply(lambda x: remove_stopwords(x))\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Name Pre-Processing (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make everything lowercase\n",
    "product_full_name = data1['product_full_name'].str.lower()\n",
    "# Remove hyphen\n",
    "product_full_name = product_full_name.str.replace(\"-\",\"\",case=False)\n",
    "# Remove numbers\n",
    "product_full_name = product_full_name.str.replace(r'[0-9]+',\"\",case=False)\n",
    "# Remove special characters and stop words\n",
    "import nltk\n",
    "stopwords = ['&','|',',','.',\"'\",'in','le','mm','and','with','the','k','x','les','de','of',\"'s\"]\n",
    "def remove_stopwords(text: str):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = list(filter(lambda token: token not in stopwords, tokens))\n",
    "    return \" \".join(filtered_tokens)\n",
    "product_full_name = product_full_name.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#function to convert nltk tag to wordnet tag\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "def lemmatize_sentence(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    wn_tagged = map(lambda x :(x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "    res_words = []\n",
    "    for word, tag in wn_tagged:\n",
    "        if tag is None:\n",
    "            res_words.append(word)\n",
    "        else:\n",
    "            res_words.append(lemmatizer.lemmatize(word,tag))\n",
    "    return \" \".join(res_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_description_lem = []\n",
    "for sentence in cleaned_descriptions:\n",
    "    cleaned_description_lem = cleaned_description_lem + [lemmatize_sentence(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_brand_lem = []\n",
    "for sentence in list(brand):\n",
    "    cleaned_brand_lem = cleaned_brand_lem + [lemmatize_sentence(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_details_lem = []\n",
    "for sentence in list(df1):\n",
    "    cleaned_details_lem = cleaned_details_lem + [lemmatize_sentence(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_productname_lem = []\n",
    "for sentence in list(product_full_name):\n",
    "    cleaned_productname_lem = cleaned_productname_lem + [lemmatize_sentence(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemm_sentence(sentence):\n",
    "    word_token = nltk.word_tokenize(sentence)\n",
    "    res_words = []\n",
    "    for word in word_token:\n",
    "        res_words.append(stemmer.stem(word))\n",
    "    return \" \".join(res_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_description_stemm = []\n",
    "for sentence in cleaned_descriptions:\n",
    "    cleaned_description_stemm = cleaned_description_stemm + [stemm_sentence(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_brand_stemm = []\n",
    "for sentence in list(brand):\n",
    "    cleaned_brand_stemm = cleaned_brand_stemm + [stemm_sentence(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_details_stemm = []\n",
    "for sentence in list(df1):\n",
    "    cleaned_details_stemm = cleaned_details_stemm + [stemm_sentence(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_productname_stemm = []\n",
    "for sentence in list(product_full_name):\n",
    "    cleaned_productname_stemm = cleaned_productname_stemm + [stemm_sentence(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,1),\n",
    "                             token_pattern=r'\\b[a-zA-Z0-9]{2,}\\b',\n",
    "                             max_df=0.8,\n",
    "                             min_df = 0.01, stop_words=updated_stopwords, max_features = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(cleaned_description_lem)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "tf_idf = tf_idf.sum(axis=1)\n",
    "description_lem_tfidf_score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "description_lem_tfidf_score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(cleaned_description_stemm)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "tf_idf = tf_idf.sum(axis=1)\n",
    "description_stemm_tfidf_score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "description_stemm_tfidf_score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brand Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(cleaned_brand_lem)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "tf_idf = tf_idf.sum(axis=1)\n",
    "brand_lem_tfidf_score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "brand_lem_tfidf_score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(cleaned_brand_stemm)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "tf_idf = tf_idf.sum(axis=1)\n",
    "brand_stemm_tfidf_score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "brand_stemm_tfidf_score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(cleaned_details_lem)\n",
    "terms = vectorizer.get_feature_names() \n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "tf_idf = tf_idf.sum(axis=1)\n",
    "details_lem_tfidf_score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "details_lem_tfidf_score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(cleaned_details_stemm)\n",
    "terms = vectorizer.get_feature_names() \n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "tf_idf = tf_idf.sum(axis=1)\n",
    "details_stemm_tfidf_score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "details_stemm_tfidf_score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Name Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(cleaned_productname_lem)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "tf_idf = tf_idf.sum(axis=1)\n",
    "productname_lem_tfidf_score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "productname_lem_tfidf_score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(cleaned_productname_stemm)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "tf_idf = tf_idf.sum(axis=1)\n",
    "productname_stemm_tfidf_score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "productname_stemm_tfidf_score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(sentence) for sentence in cleaned_description_lem ]\n",
    "description_lem_word2vec_model = Word2Vec(docs, min_count = 1, window = 5, size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(sentence) for sentence in cleaned_description_stemm]\n",
    "description_stemm_word2vec_model = Word2Vec(docs, min_count = 1, window = 5, size = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brand Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(sentence) for sentence in cleaned_brand_lem ]\n",
    "brand_lem_word2vec_model = Word2Vec(docs, min_count = 1, window = 5, size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(sentence) for sentence in cleaned_brand_stemm ]\n",
    "brand_stemm_word2vec_model = Word2Vec(docs, min_count = 1, window = 5, size = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(sentence) for sentence in cleaned_details_lem ]\n",
    "details_lem_word2vec_model = Word2Vec(docs, min_count = 1, window = 5, size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(sentence) for sentence in cleaned_details_stemm ]\n",
    "details_stemm_word2vec_model = Word2Vec(docs, min_count = 1, window = 5, size = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Name Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(sentence) for sentence in cleaned_productname_lem ]\n",
    "productname_lem_word2vec_model = Word2Vec(docs, min_count = 1, window = 5, size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(sentence) for sentence in cleaned_productname_stemm ]\n",
    "productname_stemm_word2vec_model = Word2Vec(docs, min_count = 1, window = 5, size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = pd.Series(cleaned_description_lem)+\" \"+pd.Series(cleaned_brand_lem)+\" \"+\\\n",
    "        pd.Series(cleaned_details_lem)+\" \"+pd.Series(cleaned_productname_lem)\n",
    "product = list(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(product, min_count=10)\n",
    "bi_gram = Phraser(phrases)\n",
    "product=list(bi_gram[product])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(sentence) for sentence in product]\n",
    "model = Word2Vec(docs, min_count = 1, window = 5, size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_vector = []\n",
    "for item in product:\n",
    "    temp_vector = np.zeros((1,200))\n",
    "    n = 0\n",
    "    for word in item.split(\" \"):\n",
    "        if word in model.wv.vocab:\n",
    "            temp_vector += model.wv.get_vector(word)\n",
    "        else:\n",
    "            temp_vector += np.random.normal(size=200)\n",
    "        n += 1\n",
    "    product_vector.append(temp_vector/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_category(data,threshold):\n",
    "    pred = list(map(lambda v:[i for i, e in enumerate(v) if e > threshold] ,data))\n",
    "    return np.array(pred)\n",
    "\n",
    "def pred_category_name(data,threshold):\n",
    "    pred = list(map(lambda v:[list(dummies.columns)[i] for i, e in enumerate(v) if e > threshold] ,data))\n",
    "    return np.array(pred)\n",
    "\n",
    "def actual_category(data):\n",
    "    actual=list()\n",
    "    for v in data:\n",
    "        t = []\n",
    "        for i,a in enumerate(v):\n",
    "            if a == 1:\n",
    "                t.append(i)\n",
    "        actual.append(t)\n",
    "    return actual\n",
    "\n",
    "def actual_category_name(data):\n",
    "    actual=list()\n",
    "    for v in data:\n",
    "        t = []\n",
    "        for i,a in enumerate(v):\n",
    "            if a == 1:\n",
    "                t.append(list(dummies.columns)[i])\n",
    "        actual.append(t)\n",
    "    return actual\n",
    "\n",
    "def F1(pred_result,actual_result):\n",
    "    def recall(pred_result,actual_result,attribute_value):   \n",
    "        Actual=0\n",
    "        TP=0\n",
    "\n",
    "        for i in range(len(actual_result)):\n",
    "            if actual_result[i]==attribute_value:\n",
    "                Actual+= 1\n",
    "\n",
    "                pred = set(pred_result[i])\n",
    "                true = set(actual_result[i])\n",
    "\n",
    "                if len(pred.intersection(true)) != 0:\n",
    "                    TP += 1\n",
    "\n",
    "        return TP/Actual if Actual>0 else ''\n",
    "\n",
    "\n",
    "    def Precision(pred_result,actual_result,attribute_value):   \n",
    "        Positive=0\n",
    "        TP=0\n",
    "\n",
    "        for i in range(len(pred_result)):\n",
    "            if pred_result[i]==attribute_value:\n",
    "                Positive+= 1\n",
    "\n",
    "                pred = set(pred_result[i])\n",
    "                true = set(actual_result[i])\n",
    "\n",
    "                if len(pred.intersection(true)) != 0:\n",
    "                    TP += 1\n",
    "                    \n",
    "        return TP/Positive if Positive>0 else ''\n",
    "\n",
    "    F1=pd.DataFrame(list(dummies.columns))\n",
    "    F1['attribute_name']=attribute\n",
    "    F1.columns=['attribute_value','attribute_name']\n",
    "    F1=F1[['attribute_name','attribute_value']]\n",
    "    F1['recall']=''\n",
    "    F1['Precision']=''\n",
    "    F1['F1']=''\n",
    "    for i in F1.index:\n",
    "        try:\n",
    "            F1.loc[i,'recall']=  recall(pred_result,actual_result,[F1.loc[i,'attribute_value']])\n",
    "        except:\n",
    "            continue\n",
    "    for i in F1.index:\n",
    "        try:\n",
    "            F1.loc[i,'Precision']=Precision(pred_result,actual_result,[F1.loc[i,'attribute_value']])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    for i in F1.index:\n",
    "        try:\n",
    "            F1.loc[i,'F1']=2*(F1.loc[i,'recall']*F1.loc[i,'Precision'])/(F1.loc[i,'recall']+F1.loc[i,'Precision'])\n",
    "        except:\n",
    "            continue  \n",
    "\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input - product_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({ 'input':pd.Series(product_vector),\n",
    "                    'attribute_name': list(data1['attribute_name']),\n",
    "                    'attribute_value': list(data1['new_attribute_value'])\n",
    "                   })\n",
    "\n",
    "df['attribute_value']=df['attribute_value'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~df['attribute_value'].isna()]\n",
    "dummies = pd.get_dummies(df['attribute_value'])\n",
    "data = pd.concat([df, dummies], axis=1, sort=False)\n",
    "data=data.drop('attribute_value',axis=1)\n",
    "data=data.drop('attribute_name',axis=1)\n",
    "\n",
    "X=data['input']\n",
    "y= data.iloc[:,1:].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X,y, test_size=0.25,random_state=1)\n",
    "\n",
    "vocab_size = 200\n",
    "\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Flatten, Masking\n",
    "\n",
    "max_length = 200\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_length)\n",
    "\n",
    "# define rnn model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=64, input_shape=(1, max_length)))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "# print(model.summary())\n",
    "# plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train,batch_size=200,epochs=50,verbose=0)\n",
    "\n",
    "pred_vectors_train = model.predict(X_train)\n",
    "pred_vectors_test = model.predict(X_test)\n",
    "\n",
    "model.save('product_vector_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('product_vector_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~df['attribute_value'].isna()]\n",
    "dummies = pd.get_dummies(df['attribute_value'])\n",
    "data = pd.concat([df, dummies], axis=1, sort=False)\n",
    "data=data.drop('attribute_value',axis=1)\n",
    "data=data.drop('attribute_name',axis=1)\n",
    "\n",
    "X=data['input']\n",
    "y= data.iloc[:,1:].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X,y, test_size=0.25,random_state=1)\n",
    "\n",
    "vocab_size = 200\n",
    "\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Flatten, Masking\n",
    "\n",
    "max_length = 200\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_length)\n",
    "\n",
    "# define rnn model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "model.add(SimpleRNN(units=64, input_shape=(1, max_length)))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "# print(model.summary())\n",
    "# plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train,batch_size=200,epochs=50,verbose=0)\n",
    "\n",
    "pred_vectors_train = model.predict(X_train)\n",
    "pred_vectors_test = model.predict(X_test)\n",
    "\n",
    "model.save('product_vector_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute='style'\n",
    "data=df[df['attribute_name'] == attribute]\n",
    "dummies = pd.get_dummies(data['attribute_value'])\n",
    "data = pd.concat([data, dummies], axis=1, sort=False)\n",
    "data=data.drop('attribute_value',axis=1)\n",
    "data=data.drop('attribute_name',axis=1)\n",
    "\n",
    "X=data['input']\n",
    "y= data.iloc[:,1:].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X,y, test_size=0.25,random_state=1)\n",
    "\n",
    "vocab_size = 200\n",
    "\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Flatten, Masking\n",
    "\n",
    "max_length = 200\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_length)\n",
    "\n",
    "# define rnn model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "model.add(SimpleRNN(units=64, input_shape=(1, max_length)))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "# print(model.summary())\n",
    "# plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train,batch_size=200,epochs=50,verbose=0)\n",
    "\n",
    "style_pred_vectors_train = model.predict(X_train)\n",
    "style_pred_vectors_test = model.predict(X_test)\n",
    "\n",
    "## save model\n",
    "\n",
    "# model.save('style_combine_stemm_tfidf_score_model.h5')\n",
    "model.save('style_product_vector_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.1\n",
    "style_train_pred_classes = pred_category(style_pred_vectors_train,threshold)\n",
    "style_test_pred_classes = pred_category(style_pred_vectors_test,threshold)\n",
    "\n",
    "style_train_pred_classes_name = pred_category_name(style_pred_vectors_train,threshold)\n",
    "style_test_pred_classes_name = pred_category_name(style_pred_vectors_test,threshold)\n",
    "\n",
    "style_train_true_classes = actual_category(y_train)\n",
    "style_test_true_classes = actual_category(y_test)\n",
    "\n",
    "style_train_true_classes_name = actual_category_name(y_train)\n",
    "style_test_true_classes_name = actual_category_name(y_test)\n",
    "\n",
    "style_test_F1=F1(style_test_pred_classes_name,style_test_true_classes_name)\n",
    "style_train_F1=F1(style_train_pred_classes_name,style_train_true_classes_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_test_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_train_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## occasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute='occasion'\n",
    "data=df[df['attribute_name'] == attribute]\n",
    "dummies = pd.get_dummies(data['attribute_value'])\n",
    "data = pd.concat([data, dummies], axis=1, sort=False)\n",
    "data=data.drop('attribute_value',axis=1)\n",
    "data=data.drop('attribute_name',axis=1)\n",
    "\n",
    "X=data['input']\n",
    "y= data.iloc[:,1:].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X,y, test_size=0.25,random_state=1)\n",
    "\n",
    "vocab_size = 200\n",
    "\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Flatten, Masking\n",
    "\n",
    "max_length = 200\n",
    "\n",
    "# X_train = pad_sequences(X_train, padding='post', maxlen=max_length)\n",
    "# X_test = pad_sequences(X_test, padding='post', maxlen=max_length)\n",
    "\n",
    "# define rnn model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "model.add(SimpleRNN(units=64, input_shape=(1, max_length)))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "# print(model.summary())\n",
    "# plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train,batch_size=200,epochs=50,verbose=0)\n",
    "\n",
    "occasion_pred_vectors_train = model.predict(X_train)\n",
    "occasion_pred_vectors_test = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.15\n",
    "occasion_train_pred_classes = pred_category(occasion_pred_vectors_train,threshold)\n",
    "occasion_test_pred_classes = pred_category(occasion_pred_vectors_test,threshold)\n",
    "\n",
    "occasion_train_pred_classes_name = pred_category_name(occasion_pred_vectors_train,threshold)\n",
    "occasion_test_pred_classes_name = pred_category_name(occasion_pred_vectors_test,threshold)\n",
    "\n",
    "occasion_train_true_classes = actual_category(y_train)\n",
    "occasion_test_true_classes = actual_category(y_test)\n",
    "\n",
    "occasion_train_true_classes_name = actual_category_name(y_train)\n",
    "occasion_test_true_classes_name = actual_category_name(y_test)\n",
    "\n",
    "occasion_test_F1=F1(occasion_test_pred_classes_name,occasion_test_true_classes_name)\n",
    "occasion_train_F1=F1(occasion_train_pred_classes_name,occasion_train_true_classes_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>attribute_value</th>\n",
       "      <th>recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>occasion</td>\n",
       "      <td>coldweather</td>\n",
       "      <td>0.628458</td>\n",
       "      <td>1</td>\n",
       "      <td>0.771845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>occasion</td>\n",
       "      <td>daytonight</td>\n",
       "      <td>0.987308</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.890704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>occasion</td>\n",
       "      <td>nightout</td>\n",
       "      <td>0.747259</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.83436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>occasion</td>\n",
       "      <td>vacation</td>\n",
       "      <td>0.698845</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.760187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>occasion</td>\n",
       "      <td>weekend</td>\n",
       "      <td>0.985863</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.955969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>occasion</td>\n",
       "      <td>work</td>\n",
       "      <td>0.848087</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.907126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>occasion</td>\n",
       "      <td>workout</td>\n",
       "      <td>0.79661</td>\n",
       "      <td>1</td>\n",
       "      <td>0.886792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  attribute_name attribute_value    recall Precision        F1\n",
       "0       occasion     coldweather  0.628458         1  0.771845\n",
       "1       occasion      daytonight  0.987308  0.811321  0.890704\n",
       "2       occasion        nightout  0.747259  0.944444   0.83436\n",
       "3       occasion        vacation  0.698845  0.833333  0.760187\n",
       "4       occasion         weekend  0.985863  0.927835  0.955969\n",
       "5       occasion            work  0.848087     0.975  0.907126\n",
       "6       occasion         workout   0.79661         1  0.886792"
      ]
     },
     "execution_count": 949,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occasion_train_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>attribute_value</th>\n",
       "      <th>recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>occasion</td>\n",
       "      <td>coldweather</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>occasion</td>\n",
       "      <td>daytonight</td>\n",
       "      <td>0.911265</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.272206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>occasion</td>\n",
       "      <td>nightout</td>\n",
       "      <td>0.539179</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.518851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>occasion</td>\n",
       "      <td>vacation</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>occasion</td>\n",
       "      <td>weekend</td>\n",
       "      <td>0.930956</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.506436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>occasion</td>\n",
       "      <td>work</td>\n",
       "      <td>0.683894</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.465598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>occasion</td>\n",
       "      <td>workout</td>\n",
       "      <td>0.647541</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.440111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  attribute_name attribute_value    recall Precision        F1\n",
       "0       occasion     coldweather  0.265306         0         0\n",
       "1       occasion      daytonight  0.911265      0.16  0.272206\n",
       "2       occasion        nightout  0.539179       0.5  0.518851\n",
       "3       occasion        vacation  0.483871         0         0\n",
       "4       occasion         weekend  0.930956  0.347826  0.506436\n",
       "5       occasion            work  0.683894  0.352941  0.465598\n",
       "6       occasion         workout  0.647541  0.333333  0.440111"
      ]
     },
     "execution_count": 950,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occasion_test_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subcategory_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute='subcategory_top'\n",
    "\n",
    "data=df[df['attribute_name'] == attribute]\n",
    "dummies = pd.get_dummies(data['attribute_value'])\n",
    "data = pd.concat([data, dummies], axis=1, sort=False)\n",
    "data=data.drop('attribute_value',axis=1)\n",
    "data=data.drop('attribute_name',axis=1)\n",
    "\n",
    "X=data['input']\n",
    "y= data.iloc[:,1:].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X,y, test_size=0.25,random_state=1)\n",
    "\n",
    "vocab_size = 200\n",
    "\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Flatten, Masking\n",
    "\n",
    "max_length = 200\n",
    "\n",
    "# X_train = pad_sequences(X_train, padding='post', maxlen=max_length)\n",
    "# X_test = pad_sequences(X_test, padding='post', maxlen=max_length)\n",
    "\n",
    "# define rnn model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "model.add(SimpleRNN(units=64, input_shape=(1, max_length)))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "# print(model.summary())\n",
    "# plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train,batch_size=200,epochs=50,verbose=0)\n",
    "\n",
    "top_pred_vectors_train = model.predict(X_train)\n",
    "top_pred_vectors_test = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.15\n",
    "top_train_pred_classes = pred_category(top_pred_vectors_train,threshold)\n",
    "top_test_pred_classes = pred_category(top_pred_vectors_test,threshold)\n",
    "\n",
    "top_train_pred_classes_name = pred_category_name(top_pred_vectors_train,threshold)\n",
    "top_test_pred_classes_name = pred_category_name(top_pred_vectors_test,threshold)\n",
    "\n",
    "top_train_true_classes = actual_category(y_train)\n",
    "top_test_true_classes = actual_category(y_test)\n",
    "\n",
    "top_train_true_classes_name = actual_category_name(y_train)\n",
    "top_test_true_classes_name = actual_category_name(y_test)\n",
    "\n",
    "top_test_F1=F1(top_test_pred_classes_name,top_test_true_classes_name)\n",
    "top_train_F1=F1(top_train_pred_classes_name,top_train_true_classes_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_train_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_test_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subcategory_bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x196bc770fc8>"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute='subcategory_bottom'\n",
    "data=df[df['attribute_name'] == attribute]\n",
    "dummies = pd.get_dummies(data['attribute_value'])\n",
    "data = pd.concat([data, dummies], axis=1, sort=False)\n",
    "data=data.drop('attribute_value',axis=1)\n",
    "data=data.drop('attribute_name',axis=1)\n",
    "\n",
    "X=data['input']\n",
    "y= data.iloc[:,1:].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X,y, test_size=0.25,random_state=1)\n",
    "\n",
    "vocab_size = 200\n",
    "\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Flatten, Masking\n",
    "\n",
    "max_length = 200\n",
    "\n",
    "# X_train = pad_sequences(X_train, padding='post', maxlen=max_length)\n",
    "# X_test = pad_sequences(X_test, padding='post', maxlen=max_length)\n",
    "\n",
    "# define rnn model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "model.add(SimpleRNN(units=64, input_shape=(1, max_length)))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "# print(model.summary())\n",
    "# plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train,batch_size=200,epochs=50,verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.15\n",
    "bottom_train_pred_classes = pred_category(bottom_pred_vectors_train,threshold)\n",
    "bottom_test_pred_classes = pred_category(bottom_pred_vectors_test,threshold)\n",
    "\n",
    "bottom_train_pred_classes_name = pred_category_name(bottom_pred_vectors_train,threshold)\n",
    "bottom_test_pred_classes_name = pred_category_name(bottom_pred_vectors_test,threshold)\n",
    "\n",
    "bottom_train_true_classes = actual_category(y_train)\n",
    "bottom_test_true_classes = actual_category(y_test)\n",
    "\n",
    "bottom_train_true_classes_name = actual_category_name(y_train)\n",
    "bottom_test_true_classes_name = actual_category_name(y_test)\n",
    "\n",
    "bottom_test_F1=F1(bottom_test_pred_classes_name,bottom_test_true_classes_name)\n",
    "bottom_train_F1=F1(bottom_train_pred_classes_name,bottom_train_true_classes_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_test_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_train_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>attribute_value</th>\n",
       "      <th>recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subcategory_bottom</td>\n",
       "      <td>pants&amp;leggings</td>\n",
       "      <td>1</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.979757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subcategory_bottom</td>\n",
       "      <td>shorts</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subcategory_bottom</td>\n",
       "      <td>skirts</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>1</td>\n",
       "      <td>0.968421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       attribute_name attribute_value    recall Precision        F1\n",
       "0  subcategory_bottom  pants&leggings         1  0.960317  0.979757\n",
       "1  subcategory_bottom          shorts       0.4         1  0.571429\n",
       "2  subcategory_bottom          skirts  0.938776         1  0.968421"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1(bottom_train_pred_classes_name,bottom_train_true_classes_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>attribute_value</th>\n",
       "      <th>recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subcategory_bottom</td>\n",
       "      <td>pants&amp;leggings</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.976744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subcategory_bottom</td>\n",
       "      <td>shorts</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subcategory_bottom</td>\n",
       "      <td>skirts</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       attribute_name attribute_value   recall Precision        F1\n",
       "0  subcategory_bottom  pants&leggings        1  0.954545  0.976744\n",
       "1  subcategory_bottom          shorts        0                    \n",
       "2  subcategory_bottom          skirts  0.90625         1   0.95082"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1(bottom_test_pred_classes_name,bottom_test_true_classes_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn(attribute):\n",
    "    data=df[df['attribute_name'] == attribute]\n",
    "    dummies = pd.get_dummies(data['attribute_value'])\n",
    "    data = pd.concat([data, dummies], axis=1, sort=False)\n",
    "    data=data.drop('attribute_value',axis=1)\n",
    "    data=data.drop('attribute_name',axis=1)\n",
    "\n",
    "    X=data['text']\n",
    "    y= data.iloc[:,1:].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X,y, test_size=0.25,random_state=1)\n",
    "\n",
    "    vocab_size = 200\n",
    "\n",
    "    from keras.layers.recurrent import SimpleRNN\n",
    "    from keras.layers import Flatten, Masking\n",
    "\n",
    "    max_length = 200\n",
    "\n",
    "    X_train = pad_sequences(X_train, padding='post', maxlen=max_length)\n",
    "    X_test = pad_sequences(X_test, padding='post', maxlen=max_length)\n",
    "\n",
    "    # define rnn model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(SimpleRNN(units=64, input_shape=(1, max_length)))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "    from keras.utils.vis_utils import plot_model\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    # print(model.summary())\n",
    "    # plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(X_train, y_train,batch_size=200,epochs=50,verbose=0)\n",
    "    \n",
    "    pred_vectors_train = model.predict(X_train)\n",
    "    pred_vectors_test = model.predict(X_test)\n",
    "    \n",
    "    train_pred_classes = pred_category(pred_vectors_train)\n",
    "    test_pred_classes = pred_category(pred_vectors_test)\n",
    "\n",
    "    train_pred_classes_name = pred_category_name(pred_vectors_train)\n",
    "    test_pred_classes_name = pred_category_name(pred_vectors_test)\n",
    "    \n",
    "    train_true_classes = actual_category(y_train)\n",
    "    test_true_classes = actual_category(y_test)\n",
    "\n",
    "    train_true_classes_name = actual_category_name(y_train)\n",
    "    test_true_classes_name = actual_category_name(y_test)\n",
    "    \n",
    "    test_F1=F1(test_pred_classes_name,test_true_classes_name)\n",
    "    train_F1=F1(train_pred_classes_name,train_true_classes_name)\n",
    "    \n",
    "    return train_F1,test_F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203.759px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
